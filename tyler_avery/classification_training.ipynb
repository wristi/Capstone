{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78f43421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.applications import ResNet101V2\n",
    "from keras import models, layers, optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7336502",
   "metadata": {},
   "source": [
    "# Keras Image Data Loading\n",
    "ref: https://keras.io/api/data_loading/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c1fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4e1b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(data_dir, shuffle=True):\n",
    "    # Use ImageDataGenerator for normalization\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Create a TensorFlow dataset\n",
    "    train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        color_mode='rgb',\n",
    "        image_size=(224, 224),\n",
    "        shuffle=shuffle,\n",
    "        seed=42,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        interpolation=\"bilinear\",\n",
    "        crop_to_aspect_ratio=True\n",
    "    )\n",
    "\n",
    "    val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        color_mode='rgb',\n",
    "        image_size=(224, 224),\n",
    "        shuffle=shuffle,\n",
    "        seed=42,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        interpolation=\"bilinear\",\n",
    "        crop_to_aspect_ratio=True\n",
    "    )\n",
    "\n",
    "    # Determine batch size dynamically\n",
    "    #batch_size = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    #print(f\"Using batch size: {batch_size}\")\n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c40aa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1034 files belonging to 5 classes.\n",
      "Using 828 files for training.\n",
      "Found 1034 files belonging to 5 classes.\n",
      "Using 206 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "data_directory = 'Pollen_RawData/'\n",
    "\n",
    "train_dataset,val_dataset = create_dataset(data_directory, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a8718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9656989",
   "metadata": {},
   "source": [
    "# Resnet Classification Training\n",
    "ref: https://keras.io/guides/transfer_learning/#build-a-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "090b5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee4e9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_dataset, val_dataset, num_classes, epochs=10, batch_size=32):\n",
    "    \n",
    "    \n",
    "    # Load the pre-trained ResNet50 model\n",
    "    base_model = ResNet101V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze the layers of the pre-trained model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Create a new model for transfer learning\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add the pre-trained ResNet50 model\n",
    "    model.add(base_model)\n",
    "\n",
    "    # Flatten the xoutput of the ResNet50 model\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Add your own fully connected layers for classification\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  # Categorical classification\n",
    "\n",
    "    # Compile the model with Adam optimizer & slow learning rate\n",
    "    optimizer = optimizers.Adam(lr=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', recall_m, precision_m, f1_m])\n",
    "    \n",
    "    # Callback to adjust learning rate during training\n",
    "    def lr_schedule(epoch):\n",
    "        initial_lr = 1e-4\n",
    "        decay_factor = 0.9\n",
    "        lr = initial_lr * (decay_factor ** epoch)\n",
    "        return lr\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "    # Train the model\n",
    "    history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=[lr_scheduler, tensorboard_callback])\n",
    "\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbeb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir='/logs/train',  # Directory where to write the logs\n",
    "    histogram_freq=1,   # Frequency (in epochs) at which to compute activation and weight histograms for the layers of the model\n",
    "    write_images=True   # Whether to visualize the model's graph as an image in TensorBoard\n",
    ")\n",
    "\n",
    "# During model training, pass this callback to the fit() function\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d22047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already created train_dataset and val_dataset using create_dataset\n",
    "\n",
    "#5 Output Neurons \n",
    "trained_model, training_history = train_model(train_dataset, val_dataset, epochs=10,num_classes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b605e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming you've trained your model using model.fit(...) and stored the result in 'history'\n",
    "\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65620bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(training_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d54e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc9411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac6a1231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 42s 2s/step\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Pack_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [32,5] != values[1].shape = [828] [Op:Pack] name: stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predicted_labels)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert Predictions to Class Labels\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Compute Confusion Matrix\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Pack_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [32,5] != values[1].shape = [828] [Op:Pack] name: stack"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "predictions = trained_model.predict(train_dataset)\n",
    "\n",
    "# Convert Predictions to Class Labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_label = np.argmax(true_labels)\n",
    "predicted_label = np.argmax(predicted_labels)\n",
    "\n",
    "# Convert Predictions to Class Labels\n",
    "# Compute Confusion Matrix\n",
    "conf_matrix = tf.math.confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "####CONFUSIONMATRIX STILL DOESN'T WORK, COULDNT DEBUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0d70516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_augmentation(train_dataset, val_dataset, num_classes, epochs=10, batch_size=32):\n",
    "    \n",
    "    \n",
    "    # Load the pre-trained ResNet50 model\n",
    "    base_model = ResNet101V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze the layers of the pre-trained model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Create a new model for transfer learning\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add the pre-trained ResNet50 model\n",
    "    model.add(base_model)\n",
    "\n",
    "    # Flatten the xoutput of the ResNet50 model\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Add your own fully connected layers for classification\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  # Categorical classification\n",
    "\n",
    "    # Compile the model with Adam optimizer & slow learning rate\n",
    "    optimizer = optimizers.Adam(lr=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', recall_m, precision_m, f1_m])\n",
    "    \n",
    "    # Callback to adjust learning rate during training\n",
    "    def lr_schedule(epoch):\n",
    "        initial_lr = 1e-4\n",
    "        decay_factor = 0.9\n",
    "        lr = initial_lr * (decay_factor ** epoch)\n",
    "        return lr\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    # Callback to log data for TensorBoard\n",
    "    tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1, write_images=True)\n",
    "    \n",
    "\n",
    "    model.summary()\n",
    "    # Train the model\n",
    "    history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=[lr_scheduler, tensorboard_callback])\n",
    "    \n",
    "\n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fdef6ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet101v2 (Functional)    (None, 7, 7, 2048)        42626560  \n",
      "                                                                 \n",
      " flatten_19 (Flatten)        (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 512)               51380736  \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,139,909\n",
      "Trainable params: 51,513,349\n",
      "Non-trainable params: 42,626,560\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - ETA: 0s - loss: 474.6610 - accuracy: 0.2609 - recall_m: 0.2612 - precision_m: 0.2612 - f1_m: 0.2612"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 16:55:55.746226: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [206]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2024-02-21 16:55:55.746368: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [206]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 77s 3s/step - loss: 474.6610 - accuracy: 0.2609 - recall_m: 0.2612 - precision_m: 0.2612 - f1_m: 0.2612 - val_loss: 124.1966 - val_accuracy: 0.2524 - val_recall_m: 0.2551 - val_precision_m: 0.2551 - val_f1_m: 0.2551 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 79s 3s/step - loss: 99.6663 - accuracy: 0.3744 - recall_m: 0.3740 - precision_m: 0.3740 - f1_m: 0.3740 - val_loss: 7.6125 - val_accuracy: 0.5485 - val_recall_m: 0.5459 - val_precision_m: 0.5508 - val_f1_m: 0.5483 - lr: 9.0000e-05\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 82s 3s/step - loss: 4.7723 - accuracy: 0.3998 - recall_m: 0.2809 - precision_m: 0.5399 - f1_m: 0.3418 - val_loss: 1.4322 - val_accuracy: 0.3689 - val_recall_m: 0.0950 - val_precision_m: 0.9429 - val_f1_m: 0.1707 - lr: 8.1000e-05\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 79s 3s/step - loss: 1.5682 - accuracy: 0.2899 - recall_m: 0.0795 - precision_m: 0.7761 - f1_m: 0.1399 - val_loss: 1.4857 - val_accuracy: 0.2816 - val_recall_m: 0.0759 - val_precision_m: 0.6939 - val_f1_m: 0.1338 - lr: 7.2900e-05\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 81s 3s/step - loss: 1.5380 - accuracy: 0.2766 - recall_m: 0.0685 - precision_m: 0.6904 - f1_m: 0.1218 - val_loss: 1.5228 - val_accuracy: 0.2427 - val_recall_m: 0.0548 - val_precision_m: 0.8571 - val_f1_m: 0.1021 - lr: 6.5610e-05\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 82s 3s/step - loss: 1.5324 - accuracy: 0.2681 - recall_m: 0.0642 - precision_m: 0.7891 - f1_m: 0.1172 - val_loss: 1.4628 - val_accuracy: 0.3544 - val_recall_m: 0.1633 - val_precision_m: 0.8937 - val_f1_m: 0.2740 - lr: 5.9049e-05\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 82s 3s/step - loss: 1.5251 - accuracy: 0.3019 - recall_m: 0.0953 - precision_m: 0.8490 - f1_m: 0.1652 - val_loss: 1.4897 - val_accuracy: 0.2718 - val_recall_m: 0.0625 - val_precision_m: 0.8571 - val_f1_m: 0.1148 - lr: 5.3144e-05\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 81s 3s/step - loss: 1.4662 - accuracy: 0.3092 - recall_m: 0.1061 - precision_m: 0.8810 - f1_m: 0.1861 - val_loss: 1.4450 - val_accuracy: 0.3058 - val_recall_m: 0.0950 - val_precision_m: 0.9524 - val_f1_m: 0.1703 - lr: 4.7830e-05\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 80s 3s/step - loss: 1.5242 - accuracy: 0.2983 - recall_m: 0.0925 - precision_m: 0.7919 - f1_m: 0.1604 - val_loss: 1.5706 - val_accuracy: 0.2184 - val_recall_m: 0.0223 - val_precision_m: 0.7143 - val_f1_m: 0.0433 - lr: 4.3047e-05\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 80s 3s/step - loss: 1.5734 - accuracy: 0.2319 - recall_m: 0.0232 - precision_m: 0.4103 - f1_m: 0.0435 - val_loss: 1.3958 - val_accuracy: 0.3398 - val_recall_m: 0.1422 - val_precision_m: 0.9762 - val_f1_m: 0.2448 - lr: 3.8742e-05\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already created train_dataset and val_dataset using create_dataset\n",
    "\n",
    "#5 Output Neurons \n",
    "augmented_trained_model, augmented_training_history = train_model_augmentation(train_dataset, val_dataset, epochs=10,num_classes=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50debde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished with 23% Accuracy\n",
    "# 1.57 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233931a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a55ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05864d18",
   "metadata": {},
   "source": [
    "Running Classification on Lice Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e7a3501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1239 files belonging to 5 classes.\n",
      "Using 992 files for training.\n",
      "Found 1239 files belonging to 5 classes.\n",
      "Using 247 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "data_directory = 'Lice_dataset/'\n",
    "\n",
    "lice_train,lice_val = create_dataset(data_directory, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef6cf43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet101v2 (Functional)    (None, 7, 7, 2048)        42626560  \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 512)               51380736  \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,139,909\n",
      "Trainable params: 51,513,349\n",
      "Non-trainable params: 42,626,560\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2024-02-15 12:35:51.694406: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [992]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-02-15 12:35:51.694866: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [992]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 645.2692 - accuracy: 0.2127 - recall_m: 0.2127 - precision_m: 0.2127 - f1_m: 0.2127"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 12:37:08.194588: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [247]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2024-02-15 12:37:08.194841: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [247]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 121s 4s/step - loss: 645.2692 - accuracy: 0.2127 - recall_m: 0.2127 - precision_m: 0.2127 - f1_m: 0.2127 - val_loss: 94.7939 - val_accuracy: 0.3441 - val_recall_m: 0.3427 - val_precision_m: 0.3427 - val_f1_m: 0.3427 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 120s 4s/step - loss: 105.4668 - accuracy: 0.3034 - recall_m: 0.3014 - precision_m: 0.3032 - f1_m: 0.3022 - val_loss: 2.8230 - val_accuracy: 0.2632 - val_recall_m: 0.1873 - val_precision_m: 0.3349 - val_f1_m: 0.2393 - lr: 9.0000e-05\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 111s 4s/step - loss: 2.2561 - accuracy: 0.3085 - recall_m: 0.0353 - precision_m: 0.1328 - f1_m: 0.0467 - val_loss: 1.6081 - val_accuracy: 0.3117 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_f1_m: 0.0000e+00 - lr: 8.1000e-05\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 109s 4s/step - loss: 1.6290 - accuracy: 0.3226 - recall_m: 0.0060 - precision_m: 0.1290 - f1_m: 0.0115 - val_loss: 1.6067 - val_accuracy: 0.3117 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_f1_m: 0.0000e+00 - lr: 7.2900e-05\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 109s 4s/step - loss: 1.5982 - accuracy: 0.3236 - recall_m: 0.0050 - precision_m: 0.1290 - f1_m: 0.0097 - val_loss: 1.6054 - val_accuracy: 0.3117 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_f1_m: 0.0000e+00 - lr: 6.5610e-05\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 110s 4s/step - loss: 1.6296 - accuracy: 0.3256 - recall_m: 0.0071 - precision_m: 0.1774 - f1_m: 0.0135 - val_loss: 1.6042 - val_accuracy: 0.3117 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_f1_m: 0.0000e+00 - lr: 5.9049e-05\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 111s 4s/step - loss: 1.5939 - accuracy: 0.3236 - recall_m: 0.0050 - precision_m: 0.1613 - f1_m: 0.0098 - val_loss: 1.6033 - val_accuracy: 0.3117 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_f1_m: 0.0000e+00 - lr: 5.3144e-05\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 108s 4s/step - loss: 1.6050 - accuracy: 0.3226 - recall_m: 0.0040 - precision_m: 0.1290 - f1_m: 0.0078 - val_loss: 1.6024 - val_accuracy: 0.3117 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_f1_m: 0.0000e+00 - lr: 4.7830e-05\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 110s 4s/step - loss: 1.5931 - accuracy: 0.3226 - recall_m: 0.0040 - precision_m: 0.1290 - f1_m: 0.0078 - val_loss: 1.6016 - val_accuracy: 0.3117 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_f1_m: 0.0000e+00 - lr: 4.3047e-05\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 110s 4s/step - loss: 1.5919 - accuracy: 0.3226 - recall_m: 0.0040 - precision_m: 0.1290 - f1_m: 0.0078 - val_loss: 1.6010 - val_accuracy: 0.3117 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_f1_m: 0.0000e+00 - lr: 3.8742e-05\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already created train_dataset and val_dataset using create_dataset\n",
    "\n",
    "#5 Output Neurons \n",
    "trained_model, training_history = train_model(lice_train, lice_val, epochs=10,num_classes=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54acb685",
   "metadata": {},
   "source": [
    "32% Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632e51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
